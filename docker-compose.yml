version: '3.8'

x-healthcheck:
  healthcheck:
    &healthcheck-common
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

services:
  broker:
    image: confluentinc/cp-kafka:7.9.1          # أو 8.0.0
    hostname: broker
    container_name: broker
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:29093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092   # ← مهم جدًا
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@broker:29093
      CLUSTER_ID: YmViZDBmOWJhYzNiNDQ3Yz
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    volumes:
      - broker:/var/lib/kafka/data
    networks:
      - lp
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5

  producer:
    container_name: besha-accesslog-producer   # اسم جديد واضح وما يتكررش
    build:
      context: ./producer
      dockerfile: Dockerfile-Producer
    command: ["python", "producer.py"]
    depends_on:
      - broker
    env_file:
      - ./env/kafka.env
    networks:
      - lp
    restart: always
    volumes:
      - //c/Users/besha/OneDrive/Desktop/grad/access.log:/data/access.log:ro

  spark-master:
    image: apache/spark:3.5.3
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - lp
    restart: always

  spark-worker:
    image: apache/spark:3.5.3
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - lp
    restart: always

  redis:
    image: redis:7.2.10-bookworm
    ports:
      - "6379:6379"
    networks:
      - lp
    restart: always
    healthcheck:
      <<: *healthcheck-common
      test: ["CMD", "redis-cli", "ping"]

  postgres:
    image: postgres:17.4-bookworm
    env_file:
      - ./env/postgres.env
      - ./env/postgres.creds
    ports:
      - "5432:5432"
    networks:
      - lp
    restart: always
    volumes:
      - postgres:/var/lib/postgresql/data

  minio:
    image: minio/minio:RELEASE.2025-07-18T21-56-31Z
    command: server /data --console-address ":9001"
    env_file:
      - ./env/minio.creds
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - lp
    restart: always
    volumes:
      - minio:/data

  cassandra:
    image: cassandra:5.0
    container_name: grad-cassandra
    hostname: cassandra
    ports:
      - "9042:9042"      # CQL native port (لـ Spark والتطبيقات)
      - "7199:7199"      # JMX (monitoring)
      - "9160:9160"      # Thrift (اختياري)
    environment:
      - CASSANDRA_CLUSTER_NAME=besha-cluster
      - HEAP_NEWSIZE=512M
      - MAX_HEAP_SIZE=2G
      - CASSANDRA_DC=DC1
      - CASSANDRA_RACK=RACK1
    volumes:
      - cassandra-data:/var/lib/cassandra
    healthcheck:
      test: ["CMD", "cqlsh", "-u", "cassandra", "-p", "cassandra", "-e", "describe keyspaces"]
      interval: 15s
      timeout: 10s
      retries: 10
    restart: always
    networks:
      - lp

  grafana:
    image: grafana/grafana:11.3.0
    depends_on:
      - redis
      - postgres
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    networks:
      - lp
    restart: always
    volumes:
      - grafana:/var/lib/grafana
      - ./grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml

networks:
  lp:

volumes:
  broker:
  redis:
  postgres:
  minio:
  grafana:
  cassandra-data: